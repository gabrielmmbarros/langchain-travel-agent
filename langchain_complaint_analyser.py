# Import necessary modules from LangChain and environment tools
import openai
from langchain_openai import AzureChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough

# This function loads environment variables from .env file into the application's environment,
# allowing for secure management of sensitive data
load_dotenv()

# Configure the OpenAI API with Azure credentials from environment variables
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")
openai.api_type = os.getenv("AZURE_OPENAI_API_TYPE")

# Initializing the language model with LangChain
# This creates a ChatOpenAI instance with specific parameters for generating responses
llm = AzureChatOpenAI(
    azure_deployment = "gpt-4.1-nano",
    openai_api_version = openai.api_version,
    temperature = 0.3
)

# Define each step of the complaint analysis pipeline
step_1 = PromptTemplate.from_template("Analyze the complaint: {complaint}") | llm | StrOutputParser()
step_2 = PromptTemplate.from_template("Evaluate the sentiment of the complaint: {analysis_result}") | llm | StrOutputParser()
step_3 = PromptTemplate.from_template("Formulate a response: {sentiment}") | llm | StrOutputParser()

# Build the pipeline using RunnablePassthrough to pass and assign intermediate results
pipeline = (
    {"complaint": RunnablePassthrough()}
    | RunnablePassthrough.assign(analysis_result=step_1)
    | RunnablePassthrough.assign(sentiment=step_2)
    | step_3
)

# Example complaint text to be analyzed
complaint_text = (
    "Today I bought a new phone, model X with 256 GB and flip. However, the product had a defect in the hinge and does not stay closed. "
    "Support does not answer me and I am very disappointed."
)

# Run the pipeline with the complaint text
result = pipeline.invoke({"complaint": complaint_text})

# Print the final response generated by the pipeline
print(result)